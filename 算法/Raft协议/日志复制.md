### 日志复制

共识算法通常基于**状态复制机（Replicated State Machine）模型，所有节点从同一个 state 出发**，经过一系列**同样操作 log** 的步骤，最终也必将达到**一致的 state**。也就是说，只要我们保证集群中所有节点的 log 一致，那么经过一系列追加操作（apply）后最终得到的状态机也就是一致的。

Raft 负责保证集群中所有节点 **log 的一致性**。

在 raft 里所有操作（log）都必须交给 leader 节点处理（follewer 接收写操作会转交给 leader 处理），并由 leader 节点复制给其它节点，来保证整个集群的 log 实现层面的一致。



### 复制机制

整体流程如下：

- Leader 为客户端提供服务，客户端的每个请求都包含一条即将被状态复制机执行的指令。
- Leader 把该指令作为一条新的日志附加到自身的日志集合，然后向其它节点发起**附加条目请求（AppendEntries RPC）**，来要求它们将这条日志附加到各自本地的日志集合。
- 当这条日志已经确保被**安全的复制**，即大多数（N/2+1）节点都已经复制后，leader 会将该日志 **apply** 到它本地的状态机中，然后把操作成功的结果返回给客户端。
- 此后， leader 会继续发送带有已安全复制的心跳包给 follows，告诉 follow 已经 **commited** ，follow 收到后会提交自己的日志。



<div align=middle><img src=".images/11bd5a3d291b43d2b31fea3a75fb3655~tplv-k3u1fbpfcp-watermark.image" width="50%" height="50%" /></div>

**图 1*

如图1，S1 当选 leader，此时还没有任何日志。我们模拟客户端向 S1 发起一个请求。

<div align=middle><img src=".images/7a17a0e447534541a520ec97a9942cdd~tplv-k3u1fbpfcp-watermark.image" width="50%" height="50%" /></div> 

**图 2*

如图2，S1 收到客户端请求后新增了一条日志 (term2, index1)，然后并行地向其它节点发起 AppendEntries RPC。

<div align=middle><img src=".images/2b0c350086014a52a4989f02f5262153~tplv-k3u1fbpfcp-watermark.image" width="50%" height="50%" /></div>
 **图 3*

如图3，S2、S4 率先收到了请求，各自附加了该日志，并向 S1 回应响应。

<div align=middle><img src=".images/6f062cd961a24c5a951f2c025f065bd4~tplv-k3u1fbpfcp-watermark.image" width="50%" height="50%" /></div> **图 4*

如图4，所有节点都附加了该日志，但由于 leader 尚未收到任何响应，因此暂时还不清楚该日志到底是否被成功复制。

<div align=middle><img src=".images/97dbbaaf72cc4a2ba148cb252bce95ec~tplv-k3u1fbpfcp-watermark.image" width="50%" height="50%" /></div> **图 5*

如图5，当 S1 收到**2个节点**的响应时，该日志条目的边框就已经变为实线，表示该日志已经**安全的复制**，因为在5节点集群中，2个 follower 节点加上 leader 节点自身，副本数已经确保过半，此时 **S1 将响应客户端的请求**。

<div align=middle><img src=".images/a127762b466a48719967d52ab9f0636c~tplv-k3u1fbpfcp-watermark.image" width="50%" height="50%" /></div> **图 6*

如图6，leader 后续会持续发送心跳包给 followers，心跳包中会携带当前**已经安全复制（我们称之为 committed）的日志索引**，此处为 (term2, index1)。

<div align=middle><img src=".images/f7ebe584867b4c0b98b1e46f595a5007~tplv-k3u1fbpfcp-watermark.image" width="50%" height="50%" /></div> **图 7*

如图7，所有 follower 都通过心跳包得知 (term2, index1) 的 log 已经成功复制 （committed），因此所有节点中该日志条目的边框均变为实线。



### 日志一致性的保证

Raft 保证：**如果不同的节点日志集合中的两个日志条目拥有相同的 term 和 index，那么它们一定存储了相同的指令。**

为什么可以作出这种保证？因为 raft 要求 leader 在一个 term 内针对同一个 index 只能创建一条日志，并且永远不会修改它。



同时也保证：**如果不同的节点日志集合中的两个日志条目拥有相同的 term 和 index，那么它们之前的所有日志条目也全部相同。**

这是因为 leader 发出的 AppendEntries RPC 中会额外携带**上一条**日志的 (term, index)，如果 follower 在本地找不到相同的 (term, index) 日志，则**拒绝接收这次新的日志**。

所以，只要 follower 持续正常地接收来自 leader 的日志，那么就可以通过归纳法验证上述结论。



### 日志不一致场景

在所有节点正常工作的时候，leader 和 follower的日志总是保持一致，AppendEntries RPC 也永远不会失败。然而我们总要面对任意节点随时可能宕机的风险，如何在这种情况下继续保持集群日志的一致性才是我们真正要解决的问题。

<div align=middle><img src=".images/012758dde1ba4305b417d860236a2ecc~tplv-k3u1fbpfcp-watermark.image" width="70%" height="70%" /></div>

*日志不一致场景图*



上图展示了一个 term8 的 leader 刚上任时，集群中日志可能存在的混乱情况。例如 follower 可能缺少一些日志（a ~ b），可能多了一些未提交的日志（c ~ d），也可能既缺少日志又多了一些未提交日志（e ~ f）。

*注：Follower 不可能比 leader 多出一些已提交（committed）日志，这一点是通过选举上的限制来达成的，会在下一篇 Safety 部分介绍。*

我们先来尝试复现上述 a ~ f 场景，最后再讲 raft 如何解决这种不一致问题。

**场景a~b. Follower 日志落后于 leader**

这种场景其实很简单，即 **follower 宕机了一段时间**，follower-a 从收到 (term6, index9) 后开始宕机，follower-b 从收到 (term4, index4) 后开始宕机。这里不再赘述。

**场景c. Follower 日志比 leader 多 term6**

当 term6 的 leader 正在将 (term6, index11) 向 follower 同步时，该 leader 发生了宕机，且此时只有 follower-c 收到了这条日志的 AppendEntries RPC。然后经过一系列的选举，term7 可能是选举超时，也可能是 leader 刚上任就宕机了，最终 term8 的 leader 上任了，成就了我们看到的场景 c。

**场景d. Follower 日志比 leader 多 term7**

当 term6 的 leader 将 (term6, index10) 成功 commit 后，发生了宕机。此时 term7 的 leader 走马上任，连续同步了两条日志给 follower，然而还没来得及 commit 就宕机了，随后集群选出了 term8 的 leader。

**场景e. Follower 日志比 leader 少 term5 ~ 6，多 term4**

当 term4 的 leader 将 (term4, index7) 同步给 follower，且将 (term4, index5) 及之前的日志成功 commit 后，发生了宕机，紧接着 follower-e 也发生了宕机。这样在 term5~7 内发生的日志同步全都被 follower-e 错过了。当 follower-e 恢复后，term8 的 leader 也刚好上任了。

**场景f. Follower 日志比 leader 少 term4 ~ 6，多 term2 ~ 3**

当 term2 的 leader 同步了一些日志（index4 ~ 6）给 follower 后，尚未来得及 commit 时发生了宕机，但它很快恢复过来了，又被选为了 term3 的 leader，它继续同步了一些日志（index7~11）给 follower，但同样未来得及 commit 就又发生了宕机，紧接着 follower-f 也发生了宕机，当 follower-f 醒来时，集群已经前进到 term8 了。



### 如何处理日志不一致

**Raft 强制要求 follower 必须复制 leader 的日志集合来解决不一致问题。**

也就是说，follower 节点上任何与 leader 不一致的日志，都会被 leader 节点上的日志所覆盖。这并不会产生什么问题，因为某些选举上的限制，如果 follower 上的日志与 leader 不一致，比 leader 的多。那么该日志在 follower 上**一定是未提交的**。未提交的日志并不会应用到状态机，也不会被外部的客户端感知到。

要使得 follower 的日志集合跟自己保持完全一致，leader 必须先找到二者间**最后一次**达成一致的地方。因为一旦这条日志达成一致，在这之前的日志一定也都一致。这个确认操作是在 AppendEntries RPC 的一致性检查步骤完成的。


